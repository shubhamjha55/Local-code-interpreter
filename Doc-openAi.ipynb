{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from langchain.chains import ConversationalRetrievalChain, RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import constants\n",
    "os.environ[\"OPENAI_API_KEY\"] = constants.APIKEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable to save to disk & reuse the model (for repeated queries on the same data)\n",
    "PERSIST = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entering the query\n",
    "query = input(\"Enter Your Query: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we want to reuse the model \n",
    "if PERSIST and os.path.exists(\"persist\"):\n",
    "  print(\"Reusing index...\\n\")\n",
    "  vectorstore = Chroma(persist_directory=\"persist\", embedding_function=OpenAIEmbeddings())\n",
    "  index = VectorStoreIndexWrapper(vectorstore=vectorstore)\n",
    "# Otherwise\n",
    "else:\n",
    "  # Providing the files to loader\n",
    "  loader = DirectoryLoader(\"data.txt\")\n",
    "  if PERSIST:\n",
    "    index = VectorstoreIndexCreator(vectorstore_kwargs={\"persist_directory\":\"persist\"}).from_loaders([loader])\n",
    "  else:\n",
    "    index = VectorstoreIndexCreator().from_loaders([loader])\n",
    "\n",
    "chain = ConversationalRetrievalChain.from_llm(\n",
    "  llm=ChatOpenAI(model=\"gpt-3.5-turbo\"),\n",
    "  retriever=index.vectorstore.as_retriever(search_kwargs={\"k\": 1}),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "while True:\n",
    "  if not query:\n",
    "    query = input(\"Enter Your Query: \")\n",
    "  if query in ['quit', 'q', 'exit']:\n",
    "    sys.exit()\n",
    "  result = chain({\"question\": query, \"chat_history\": chat_history})\n",
    "  print(result['answer'])\n",
    "\n",
    "  chat_history.append((query, result['answer']))\n",
    "  query = None"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
